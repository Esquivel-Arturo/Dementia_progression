\section{Transition matrix}
The rates matrix R has non-diagonal elements of 0, for transitions that
cannot occur, or $X\beta$; there
is a separate column of $\beta$ for each non-zero element.  
(If there were no common coefficients across rates, then each row
of the $\beta$ matrix would have only 1 non-zero element.)
The transition matrix is $P= \exp(Rt)$ where $t$ is the time interval.

The matrix exponential is a hard problem, formally defined as
\begin{equation*}
  \exp(R) = I + \sum_{j=1}^\infty R^i/i!
  \end{equation*}
The computation is nicely solved by the expm package
\emph{if} we didn't need derivatives and/or high speed.  We want both.
The key idea is from a paper by Kalbfleisch and Lawless: if the rate matrix
$R$ has distinct eigenvalues then $\exp(R)$ and its derivative can be written
in terms of a matrix decomposition.  
If $R$ is upper triangular this decompostion is itself very easy to compute.

Nearly all the rate matrices encountered during an iteration will have
distint eigenvalues.
For the few which do not we can use the Pade' approximation found in the
\code{expm.higham08}.
If the rate matrix is upper triangular (\code{uppertri}), then the
eigenvalues are found on the diagonal of $R$.  
The overall stategy is the following:
\begin{itemize}
  \item Do not need derivatives: 
    call C code for all the rates at once.  That code uses
      the C call to expm for matrices it can't handle.
  \item Do need derivatives
    \begin{itemize}
      \item If upper triangular and no tied values, use the derive routine
      \item Otherwise use the Pade routine
    \end{itemize}
\end{itemize}

\subsection{Decompostion}
Based on Kalbfleisch and Lawless, ``The analysis of panel data under a 
Markov assumption'' (J Am Stat Assoc, 1985:863-871), the
rate matrix $R$ can be written as $ADA^{-1}$ for some matrix $A$, where
$D$ is a diagonal matrix of eigenvalues, provided all of the eigenvalues
are distinct.
Then $\exp(R) = A \exp(D) A^{-1}$, and the exponential of a diagonal
matrix is simply a diagonal matrix of the exponentials.
The matrix $Rt$ for a scalar $t$ has decomposition $A(Dt)A^{-1}$; a
single decompostion suffices for all values of $t$.

Because we don't allow backwards transitions our rate matrix can be made
upper triangular, simply put the states in the right order.
A particular example is
\begin{equation}
  R =
  \begin{pmatrix}
    r_{11} & r_{12} & r_{13} & 0 & 0 & r_{15}\\
    0 & r_{22} & 0 & r_{24} & 0 & r_{25}\\
    0 & 0 & r_{33} & r_{34} & r_{35} & r_{35}\\
    0 & 0 & 0 & r_{44} & r_{45} & r_{45} \\
    0 & 0 & 0 & 0 & r_{55} & r_{55} \\
    0 & 0 & 0 & 0 & 0 & 0
  \end{pmatrix}.
\end{equation}
Since this is a transition matrix the diagonal elements are constrained so that
row sums are zero: $r_{ii} = -\sum_{j\ne i} r_{ij}$.
Since R is an upper triangular matrix it's eigenvalues lie on the diagonal.
If none of the the eigenvalues are
repeated, then the Prentice result applies.

The decompostion is quite simple since $R$ is triangular.
We want the eigenvectors, i.e. solutions to 
\begin{align*}
  R v_i &= r_{ii} v_i \\
%  R v_2 &= r_{22} v_2 \\
%  R v_3 &= r_{33} v_3 \\
%  R v_4 &= r_{44} v_4 \\
%  R v_5 &= r_{55} v_5 \\
%  R v_6 &= r_{66} v_6
\end{align*}
for $i= 1, \dots, 6$, where $v_i$ are the colums of $V$. 

A sneaky fact is that the set of eigenvectors is
also upper triangular; we can solve for them one by one
using back substitution.
For the first eigenvector we have 
$v_1 = (1, 0,0,0,0,0)$.
For the second we have the equations
\begin{align*}
  r_{11} x + r_{12}y &=  r_{22} x \\
             r_{22}y &=  r_{22} y
\end{align*}
which has the solution $(r_{12}/(r_{22}- r_{11}), 1, 0,0,0,0)$,
and the process recurs for other rows.
Since $V$ is triangular the inverse of $V$ is upper triangular
and also easy to compute.

This approach fails if there are tied eigenvalues.
Kalbfleice and Lawless comment that this case is rare,
but one can then use a decomposition to Jordan canonical form re
Cox and Miller, the Theory of Stochastic Processes, 1965.
Although this leads to some nice theorems it does not give a 
simple comutational form, however, 
and it is easier to fall back on the expm() routine.

Given a $p$ by $p$ rate matrix, this computes the decompostion.
The decomp routine and its C code are used for testing.
<<decomp>>=
decomp <- function(rmat, time, eps=1e-6) {
    delta <- diff(sort(diag(rmat)))
    if (any(delta < eps) || any(rmat[row(rmat) > col(rmat)] >0)) 
        stop("invalid matrix")
    else .Call("cdecomp", rmat, time)
}
@ 

The routine below is modeled after the cholesky routines in the survival
library.  
To help with notation, the return values are labeled as in the 
Kalbfleisch and Lawless paper,
except that their Q = our rmat.  Q = A diag(d) Ainv and P= exp(Qt)

<<cdecomp>>=
/*
** Compute the eigenvectors for the upper triangular matrix R
*/
#include <math.h>
#include "R.h"
#include "Rinternals.h"

SEXP cdecomp(SEXP R2, SEXP time2) {
    int i,j,k;
    int nc, ii;
    
    static const char *outnames[]= {"d", "A", "Ainv", 
				    "P", ""};    
    SEXP rval, stemp;
    double *R, *A, *Ainv, *P;
    double *dd, temp, *ediag;
    double time;

    nc = ncols(R2);   /* number of columns */
    R = REAL(R2);
    time = asReal(time2);

    /* Make the output matrices as copies of R, so as to inherit
    **   the dimnames and etc
    */
    
    PROTECT(rval = mkNamed(VECSXP, outnames));
    stemp=  SET_VECTOR_ELT(rval, 0, allocVector(REALSXP, nc));
    dd = REAL(stemp);
    stemp = SET_VECTOR_ELT(rval, 1, allocMatrix(REALSXP, nc, nc));
    A = REAL(stemp);
    for (i =0; i< nc*nc; i++) A[i] =0;   /* R does not zero memory */
    stemp = SET_VECTOR_ELT(rval, 2, duplicate(stemp));
    Ainv = REAL(stemp);
    stemp = SET_VECTOR_ELT(rval, 3, duplicate(stemp));
    P = REAL(stemp);
   
    ediag = (double *) R_alloc(nc, sizeof(double));
    
    /* 
    **	Compute the eigenvectors
    **   For each column of R, find x such that Rx = kx
    **   The eigenvalue k is R[i,i], x is a column of A
    **  Remember that R is in column order, so the i,j element is in
    **   location i + j*nc
    */
    ii =0; /* contains i * nc */
    for (i=0; i<nc; i++) { /* computations for column i */
	dd[i] = R[i +ii];    /* the i,i diagonal element = eigenvalue*/
	A[i +ii] = 1.0;
	for (j=(i-1); j >=0; j--) {  /* fill in the rest */
	    temp =0;
	    for (k=j; k<=i; k++) temp += R[j + k*nc]* A[k +ii];
	    A[j +ii] = temp/(dd[i]- R[j + j*nc]);
	}
	ii += nc;
    }
    
    /*
    ** Solve for A-inverse, which is also upper triangular. The diagonal
    **  of A and the diagonal of A-inverse are both 1.  At the same time 
    **  solve for P = A D Ainverse, where D is a diagonal matrix 
    **  with exp(eigenvalues) on the diagonal.
    ** P will also be upper triangular, and we can solve for it using
    **  nearly the same code as above.  The prior block had RA = x with A the
    **  unknown and x successive colums of the identity matrix. 
    **  We have PA = AD, so x is successively columns of AD.
    ** Imagine P and A are 4x4 and we are solving for the second row
    **  of P.  Remember that P[2,1]= A[2,3] = A[2,4] =0; the equations for
    **  this row of P are:
    **
    **    0*A[1,2] + P[2,2]A[2,2] + P[2,3] 0     + P[2,4] 0     = A[2,2] D[2]
    **    0*A[1,3] + P[2,2]A[2,3] + P[2,3]A[3,3] + P[2,4] 0     = A[2,3] D[3]
    **    0*A[1,4] + P[2,2]A[2,4] + P[2,3]A[3,4] + P[2,4]A[4,4] = A[2,4] D[4]
    **
    **  For A-inverse the equations are (use U= A-inverse for a moment)
    **    0*A[1,2] + U[2,2]A[2,2] + U[2,3] 0     + U[2,4] 0     = 1
    **    0*A[1,3] + U[2,2]A[2,3] + U[2,3]A[3,3] + U[2,4] 0     = 0
    **    0*A[1,4] + U[2,2]A[2,4] + U[2,3]A[3,4] + U[2,4]A[4,4] = 0
    */
    
    ii =0; /* contains i * nc */
    for (i=0; i<nc; i++) ediag[i] = exp(time* dd[i]);
    for (i=0; i<nc; i++) { 
	/* computations for column i of A-inverse */
	Ainv[i+ii] = 1.0 ;
	for (j=(i-1); j >=0; j--) {  /* fill in the rest of the column*/
	    temp =0;
	    for (k=j+1; k<=i; k++) temp += A[j + k*nc]* Ainv[k +ii];
	    Ainv[j +ii] = -temp;
	}
	
        /* column i of P */
	P[i + ii] = ediag[i];
        for (j=0; j<i; j++) {
	    temp =0;
            for (k=j; k<nc; k++) temp += A[j + k*nc] * Ainv[k+ii] * ediag[k];
            P[j+ii] = temp;
        }
        
	/* alternate computations for row i of P, does not use Ainv*/
	/*P[i +ii] = ediag[i];
	  for (j=i+1; j<nc; j++) { 
	      temp =0;
	      for (k=i; k<j; k++) temp += P[i+ k*nc]* A[k + j*nc];
              P[i + j*nc] = (A[i + j*nc]*ediag[j] - temp)/A[j + j*nc];
	  } 
        */
	ii += nc;
    }
    UNPROTECT(1);
    return(rval);
}
@ 

\subsection{Derivatives}
From Kalbfliesch and Lawless, the first derivative of 
$P = \exp(Rt)$ is
\begin{align*}
  \frac{\partial P}{\partial \theta} &= AVA^{-1} \\
     V_{ij} &= \left\{ \begin{array}{ll}
         G_{ij}(e^{d_i t} - e^{d_j t})/(d_i - d_j) & i \ne j \\
         G_{ii}t e^{d_it} & i=j 
         \end{array} \right. \\
       G&= A (\partial R /\partial \theta) A^{-1}
\end{align*}
The formula for the off diagonal elements collapses to give the formula for
the diagonal ones by an application of L'Hospital's rule (for the math
geeks).

Each off diagonal element of R is $\exp(X_i\beta)= \exp(\eta_i)$ for a fixed
vector $X_i$ --- we are computing the derivative per subject and
so only one row of $X$ applies.  The first derivative with respect to
$\beta_j$ is then $X_{ij} \exp(\eta_{i})$.
Since the rows of R sum to a constant then the rows of 
its derivative must sum to zero;
we can fill in the diagonal element after the off diagonal ones are computed.
This notation has left something out: each subject has an $\eta$ vector
for each of the non-zero transitions, there is a matrix
of derivatives ($P$ is a matrix after all) for each $\beta_j$.

This computation is more bookkeeping than the earlier one, but no
single portion is particularly intensive computationally when the number
of states is modest.

The input will be the $X$ matrix row for the particular subject, 
the coefficient matrix, the rates matrix, time interval, and the mapping
vector from eta to the rates.  The last tells us where the zeros are.

<<derivative>>=
derivative <- function(rmat, time, dR, eps=1e-8) {
    ncoef <- dim(dR)[3]
    nstate <- nrow(rmat)
    dlist <- decomp(rmat, time)
    
    dmat <- array(0.0, dim=c(nstate, nstate, ncoef))
    vtemp <- outer(dlist$d, dlist$d,
                   function(a, b) {
                       ifelse(abs(a-b)< eps, time* exp(time* (a+b)/2),
                         (exp(a*time) - exp(b*time))/(a-b))})
    # any unique value of cmap appears on only one row of cmap,
    #  multiple times in that row if a coefficient is shared
    # two transitions can share a coef, but only for the same X variable
    for (i in 1:ncoef) {
        G <- dlist$Ainv %*% dR[,,i] %*% dlist$A
        V <- G*vtemp
        dmat[,,i] <- dlist$A %*% V %*% dlist$Ainv
    }
    dlist$dmat <- dmat
    dlist
    }
@ 

The following routine accepts an $X$ vector and a coefficient
matrix where $eta = X \beta$, along with the mapping matrix $Q$
that has shows which column of $\eta$ maps to which elements of
$R$ with mapping $\exp(\eta)$.  
The matrix $\code{cmap}$ is of the same shape as $\beta$, and has
zeros for those elements of $\beta$ that are fixed.
The routine returns the exponential and its derivatives with
respect to each coefficient. 

<<derivtest>>=
# This routine is used in the test suite, to call the internal derivative()
#  routine.
derivtest <- function(time, x, beta, qmat, cmap) {
    eta <- x %*% beta
    nstate <- ncol(qmat)
    ntransition <- sum(qmat > 0)
    if (ncol(eta) != ntransition) stop("ncol(beta) != ntransition")
    rindex <- which(qmat > 0)
    rmat <- matrix(0., nstate, nstate)
    rmat[qmat>0] <- exp(eta)
    diag(rmat) <- -rowSums(rmat)
    
    # The derivative oF Rmat wrt each coefficint
    nonzero <- (cmap >0)
    coeff <- unique(cmap[nonzero])
    dmat <- array(0., c(nstate, nstate, length(coeff)))
    rr <- row(cmap)[nonzero]
    cc <- col(cmap)[nonzero]
    kk  <- cmap[nonzero]
    for (i in seq_along(coeff)) {
        temp <- matrix(0, nstate, nstate)
        for (j in  which(kk == coeff[i])) {
            indx <- rindex[cc[j]]  #cc[j] = which linear predictor
            temp[indx] <- rmat[indx] * x[rr[j]]
        }
        
        diag(temp) <- -rowSums(temp)
        dmat[,,i] <- temp
    }
    derivative(rmat, time, dmat)
}

<<decomp>>
    
<<derivative>>
@ 

